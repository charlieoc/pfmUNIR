{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('pfm-unir': conda)"
  },
  "interpreter": {
   "hash": "ffcca2e828e85857b09c10fbb8dd6a909bd6f59dd80781580440e182d84704e0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "En el presente documento se estará realizando un análisis de sentimiento a las conferencias diarias que el president de México lleva a cabo diariamente. Se utilizarán específicamente las comprendidas entre el 1 de Agosto de 2019 y el 30 de Abril de 2021.\n",
    "\n",
    "Primero realizamos la importación de las librerías que se estarán utilizando durante el procesamiento."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import matplotlib\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "source": [
    "Por el momento se ha definido la utilización de un archivo de texto como fuente, por lo que procedemos a su apertura."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo de texto a utilizar como fuente\n",
    "abrir_archivo = \"/home/robert/GitHub/pfmUNIR/archivos/ConferenciasMatutinas.txt\"\n",
    "\n",
    "with open(abrir_archivo, 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "source": [
    "Continuamos con actividades de preparación del texto como es la separación en palabras, conversión a minúsculas, eliminación de signos de puntuación, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar palabras\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Transformar a minúsculas\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# Quitar puntuación\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "# Quitar otros caracteres no alfanuméricos\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "# Quitar palabras comunes (stop words)\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "words = [w for w in words if not w in stop_words]"
   ]
  },
  {
   "source": [
    "A continuación podemos comenzar a utilizar el texto transformado para realizar algunos análisis, podemos empezar con obtener la lista de las palabras más usuadas y graficarlas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd5967e8358>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Obtener la frecuencia de las palabras\n",
    "freq_dist = FreqDist(words)\n",
    "\n",
    "# Graficar las 25 más utilizadas\n",
    "freq_dist.plot(25, cumulative=False)"
   ]
  }
 ]
}